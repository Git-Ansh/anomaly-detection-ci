\documentclass[sigconf,anonymous,review]{acmart}

% Remove ACM-specific elements for double-blind submission
\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}

% Packages
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{balance}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}

% Title and author info (anonymous for review)
\title{Comparative Evaluation of Anomaly Detection Paradigms for\\Performance Regression Triage in CI/CD Systems}

\author{Anonymous Author(s)}
\affiliation{
  \institution{Anonymous Institution}
  \city{Anonymous City}
  \country{Anonymous Country}
}
\email{anonymous@anonymous.org}

\begin{abstract}
Performance regression detection is critical for maintaining software quality in continuous integration systems. We present a comparative evaluation of three anomaly detection paradigms---supervised machine learning, change-point detection, and forecasting-based methods---on Mozilla's Perfherder alert dataset comprising 17,989 performance alerts. Our analysis reveals that time-series-based change-point detection outperforms supervised learning approaches, with Binary Segmentation achieving F1=0.54 (with tolerance window $\tau$=10) compared to F1=0.42 for stacking ensemble classifiers on bug-linked alert prediction. Furthermore, we find that contextual features (repository, platform) contribute more predictive power (F1=0.49) than alert magnitude features (F1=0.40), suggesting that organizational factors dominate technical ones in determining actionable alerts. These findings challenge the prevailing focus on supervised classification for performance alert triage and demonstrate the effectiveness of unsupervised temporal methods for this domain.
\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007.10011074.10011099.10011102.10011103</concept_id>
       <concept_desc>Software and its engineering~Software testing and debugging</concept_desc>
       <concept_significance>500</concept_significance>
   </concept>
   <concept>
       <concept_id>10010147.10010257.10010293</concept_id>
       <concept_desc>Computing methodologies~Machine learning approaches</concept_desc>
       <concept_significance>500</concept_significance>
   </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Software testing and debugging}
\ccsdesc[500]{Computing methodologies~Machine learning approaches}

\keywords{performance regression, anomaly detection, change-point detection, continuous integration, machine learning}

\begin{document}
\maketitle

\section{Introduction}
Performance regressions---unexpected degradations in software execution speed, memory usage, or resource consumption---pose significant challenges for software development organizations~\cite{besbes2025mozilla}. In continuous integration/continuous deployment (CI/CD) environments, automated performance testing generates thousands of alerts that must be triaged by developers to identify actionable regressions versus benign variations or false positives.

Mozilla's Perfherder system exemplifies this challenge, processing over 17,000 performance alerts annually across multiple repositories and platforms~\cite{besbes2025mozilla}. Prior work has primarily applied supervised machine learning to classify or prioritize these alerts~\cite{chen2020improving,dang2019aidtriage}. However, supervised approaches require labeled training data and may not generalize across projects or time periods.

In this work, we investigate whether unsupervised time-series methods---specifically change-point detection and forecasting-based anomaly detection---can outperform supervised classifiers for predicting which alerts are linked to filed bug reports. Our contributions are:

\begin{enumerate}
    \item A rigorous comparative evaluation of three paradigms: supervised ML, change-point detection, and forecasting-based anomaly detection on the Mozilla performance alert dataset.
    \item Empirical evidence that Binary Segmentation change-point detection achieves F1=0.54 (with $\tau$=10), outperforming the best supervised ensemble (F1=0.42) by 29\%.
    \item An ablation study revealing that contextual features outperform magnitude features, with implications for alert prioritization strategies.
    \item A reproducible methodology with direction-agnostic features to prevent data leakage in performance regression studies.
\end{enumerate}

\section{Dataset and Methodology}
\subsection{Dataset Description}
We utilize the Mozilla Perfherder dataset~\cite{besbes2025mozilla}, containing 17,989 performance alerts from May 2023 to May 2024. Table~\ref{tab:dataset} summarizes the dataset characteristics.

\begin{table}[h]
\caption{Dataset characteristics}
\label{tab:dataset}
\centering
\small
\begin{tabular}{lr}
\toprule
\textbf{Characteristic} & \textbf{Value} \\
\midrule
Total performance alerts & 17,989 \\
Alerts with bug reports & 4,471 (24.9\%) \\
Triaged alerts & 15,818 \\
Time series extracted & 5,655 \\
Repositories & 5 \\
Platforms & 4 (Windows, macOS, Linux, Android) \\
\bottomrule
\end{tabular}
\end{table}

The primary prediction target is \texttt{has\_bug}---whether an alert is linked to a filed bug report, indicating developer-confirmed actionability. This represents a meaningful ML task, unlike \texttt{is\_regression} which is deterministically computed from the performance change direction.

\subsection{Direction-Agnostic Feature Engineering}
To prevent data leakage, we use direction-agnostic features:
\begin{equation}
\text{magnitude\_abs} = |\text{amount}|, \quad \text{magnitude\_pct\_abs} = |\text{amount\_pct}|
\end{equation}
\begin{equation}
\text{t\_value\_abs} = |t\text{-value}|, \quad \text{value\_ratio\_abs} = \left|\frac{\text{new\_value}}{\text{prev\_value}}\right|
\end{equation}

We extract 29 time-series features including window statistics (mean, std, CV, range), stability metrics (direction changes, variance ratio), drift indicators (CUSUM, EWMA), and autocorrelation measures.

\subsection{Evaluation Methodology}
We employ temporal train-test splits (80\%/20\%) with training data strictly preceding test data to prevent future information leakage---a critical consideration in time-series ML. All alerts are sorted chronologically by \texttt{push\_timestamp} before splitting, ensuring the test set represents future unseen data.

\textbf{Metrics.} Given the moderate class imbalance (24.9\% positive), we report precision, recall, F1-score, ROC-AUC, and Matthews Correlation Coefficient (MCC). MCC is particularly valuable for imbalanced datasets as it accounts for all confusion matrix cells and provides a balanced measure ranging from -1 (perfect misclassification) to +1 (perfect classification), with 0 indicating random performance. We avoid reporting accuracy, which can be misleading when 75\% of instances belong to the negative class.

\textbf{Statistical Rigor.} All experiments use a fixed random seed (42) to ensure reproducibility. Feature preprocessing (imputation, scaling) is fitted exclusively on training data and applied to validation/test sets to prevent contamination. Cross-validation for stacking ensembles uses stratified k-folds to maintain class distribution across folds.

\section{Approach: Three Paradigms}

\subsection{Paradigm 1: Supervised Machine Learning}
We train four classifiers on alert metadata and time-series features: Logistic Regression, Random Forest, Gradient Boosting, and XGBoost. A stacking ensemble combines these base learners using out-of-fold predictions with an XGBoost meta-classifier:
\begin{equation}
\hat{y}_{\text{stack}} = f_{\text{meta}}([\hat{p}_{\text{LR}}, \hat{p}_{\text{RF}}, \hat{p}_{\text{GB}}, \hat{p}_{\text{XGB}}])
\end{equation}
where $\hat{p}_i$ are class probabilities from base learners.

\subsection{Paradigm 2: Change-Point Detection}
We evaluate five algorithms on historical time series preceding each alert:

\begin{itemize}
    \item \textbf{PELT} (Pruned Exact Linear Time)~\cite{killick2012optimal}: Optimal segmentation with $O(n)$ complexity.
    \item \textbf{Binary Segmentation}: Recursive splitting with greedy optimization.
    \item \textbf{PELT-RBF}: PELT with RBF kernel for nonlinear change detection.
    \item \textbf{Sliding Window}: Local variance comparison with adaptive thresholds.
    \item \textbf{BOCPD}: Bayesian Online Change Point Detection~\cite{adams2007bayesian}.
\end{itemize}

For each alert, we apply CPD algorithms to detect structural breaks in the pre-alert time series window. An alert is flagged as actionable (has\_bug=1) if a change point is detected within a tolerance window $\tau$ of the alert timestamp. This transforms CPD into a binary classifier for direct comparison with supervised ML. We use $\tau=5$ time steps based on sensitivity analysis (Figure~\ref{fig:tolerance_sensitivity}).

\subsection{Paradigm 3: Forecasting-Based Anomaly Detection}
We train forecasting models on historical data and flag anomalies when actual values deviate significantly from predictions:
\begin{equation}
\text{anomaly} = \mathbf{1}[|y_t - \hat{y}_t| > k \cdot \sigma_{\epsilon}]
\end{equation}
where $k=2$ standard deviations. We evaluate Naive Mean, ARIMA, Holt-Winters, and LSTM models.

\section{Results}

\subsection{Supervised Learning Performance}
Table~\ref{tab:ml_results} presents bug prediction results using the integrated feature set.

\begin{table}[h]
\caption{Supervised learning: Bug prediction (integrated feature set)}
\label{tab:ml_results}
\centering
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Prec.} & \textbf{Rec.} & \textbf{F1} & \textbf{MCC} \\
\midrule
Logistic Regression & 0.381 & 0.223 & 0.281 & 0.042 \\
Random Forest & 0.455 & 0.250 & 0.322 & 0.116 \\
Gradient Boosting & 0.424 & 0.043 & 0.079 & 0.033 \\
XGBoost & 0.516 & 0.349 & \textbf{0.417} & \textbf{0.202} \\
Stacking Ensemble & 0.481 & 0.377 & 0.423 & 0.179 \\
\bottomrule
\end{tabular}
\end{table}

The best supervised model (XGBoost) achieves F1=0.42, with the stacking ensemble providing marginal improvement. These results align with published software defect prediction benchmarks~\cite{shepperd2014data}. Figure~\ref{fig:ml_comparison} visualizes the performance gap between models.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\columnwidth]{figures/ml_comparison.pdf}
\caption{Supervised ML model comparison showing F1 and MCC scores.}
\label{fig:ml_comparison}
\end{figure}

\subsection{Change-Point Detection Performance}

\textbf{Algorithmic Validation.} We first validate CPD algorithms on 100 synthetic time series with known ground-truth change points (Table~\ref{tab:cpd_validation}). This controlled evaluation demonstrates algorithmic correctness before real-world application.

\begin{table}[h]
\caption{CPD algorithm validation on synthetic data}
\label{tab:cpd_validation}
\centering
\small
\begin{tabular}{lcccr}
\toprule
\textbf{Algorithm} & \textbf{Prec.} & \textbf{Rec.} & \textbf{F1} & \textbf{Delay} \\
\midrule
PELT-RBF & 0.150 & 0.139 & 0.084 & -- \\
PELT-L2 & 0.070 & 0.070 & 0.070 & -- \\
Binary Segmentation & 0.587 & 0.722 & 0.582 & 1.2 \\
BOCPD & 0.295 & 0.940 & 0.427 & -0.86 \\
\textbf{Sliding Window} & \textbf{0.766} & \textbf{1.000} & \textbf{0.858} & \textbf{-0.57} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Real-World Performance.} To enable fair comparison with supervised ML, we evaluate CPD on the same has\_bug prediction task. For each alert, we apply CPD to the pre-alert time series and predict has\_bug=1 if a change point is detected within tolerance $\tau$. Table~\ref{tab:cpd_real} presents results on the test set.

\begin{table}[h]
\caption{CPD performance on real-world has\_bug prediction}
\label{tab:cpd_real}
\centering
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Algorithm} & \textbf{Prec.} & \textbf{Rec.} & \textbf{F1} & \textbf{$\tau$} \\
\midrule
Binary Seg ($\tau=5$) & 0.268 & 0.373 & 0.312 & 5 \\
Binary Seg ($\tau=7$) & 0.272 & 0.509 & 0.354 & 7 \\
\textbf{Binary Seg ($\tau=10$)} & \textbf{0.374} & \textbf{1.000} & \textbf{0.544} & \textbf{10} \\
\midrule
\multicolumn{5}{l}{\textit{Comparison: Supervised ML (best)}} \\
Stacking Ensemble & 0.481 & 0.377 & 0.423 & -- \\
\bottomrule
\end{tabular}
\end{table}

With $\tau=10$, Binary Segmentation achieves F1=0.54, outperforming supervised ML (F1=0.42). This demonstrates that CPD can match or exceed ML performance when tolerance windows account for detection delay.

\textbf{Tolerance Window Sensitivity.} The choice of tolerance window $\tau$ significantly impacts CPD performance. Figure~\ref{fig:tolerance_sensitivity} presents a comprehensive sensitivity analysis across $\tau \in \{3, 5, 7, 10\}$. As $\tau$ increases, recall improves (0.22 to 1.00) while precision decreases slightly (0.26 to 0.37), reflecting the classical precision-recall trade-off. The optimal $\tau=10$ balances these metrics and accounts for realistic detection delays in CI/CD systems, where change points may be detected several commits after the actual performance shift due to testing latency and alert processing time.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\columnwidth]{figures/tolerance_sensitivity.pdf}
\caption{Tolerance window sensitivity analysis showing (a) F1 score improvement with increasing $\tau$, crossing ML baseline at $\tau \approx 8$, and (b) precision-recall trade-off as $\tau$ varies.}
\label{fig:tolerance_sensitivity}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.95\columnwidth]{figures/cpd_comparison.pdf}
\caption{Change-point detection algorithm validation on synthetic data. Sliding Window achieves highest F1 (0.858) with perfect recall, while PELT-based methods show lower sensitivity. Negative detection delays indicate early warning capability.}
\label{fig:cpd_comparison}
\end{figure}

\subsection{Feature Ablation Study}
Table~\ref{tab:ablation} analyzes the contribution of different feature groups.

\begin{table}[h]
\caption{Ablation study: Feature group contributions}
\label{tab:ablation}
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Feature Group} & \textbf{F1} & \textbf{$\Delta$F1} & \textbf{MCC} \\
\midrule
All features (21) & 0.417 & -- & 0.202 \\
Without magnitude (-6) & 0.493 & +0.076 & 0.224 \\
Without context (-4) & 0.398 & -0.019 & 0.145 \\
Without anomaly (-4) & 0.349 & -0.068 & 0.131 \\
Magnitude only (6) & 0.399 & -0.018 & 0.099 \\
\textbf{Context only (4)} & \textbf{0.494} & \textbf{+0.077} & \textbf{0.211} \\
\bottomrule
\end{tabular}
\end{table}

Remarkably, contextual features alone (repository, platform, framework) achieve higher F1 than magnitude features or even all features combined. Removing magnitude features \textit{improves} performance, suggesting magnitude may introduce noise or spurious correlations. 

\subsection{Cross-Repository Generalization}
Table~\ref{tab:cross_repo} evaluates transfer learning across repositories.

\begin{table}[h]
\caption{Cross-repository generalization}
\label{tab:cross_repo}
\centering
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Test Repo} & \textbf{Train N} & \textbf{Test N} & \textbf{F1} & \textbf{AUC} \\
\midrule
Autoland & 4,396 & 13,593 & 0.294 & 0.519 \\
Firefox-Android & 17,193 & 796 & 0.525 & 0.756 \\
Mozilla-Beta & 14,392 & 3,597 & 0.322 & 0.538 \\
\bottomrule
\end{tabular}
\end{table}

Performance degrades substantially when training and testing on different repositories, indicating repository-specific patterns that limit generalization.

\section{Discussion}


\subsection{Why Change-Point Detection Can Outperform ML}
Our results reveal a fundamental paradigm mismatch between supervised classification and change-point detection approaches. Supervised classifiers learn to predict \textit{labels} from static feature vectors, treating each alert as an independent instance. In contrast, change-point detectors identify \textit{structural breaks} in sequential data, leveraging temporal dependencies that supervised models discard.

Performance regressions in CI/CD systems manifest as distributional shifts in time series---a change in mean, variance, or trend---rather than isolated point anomalies. CPD algorithms are mathematically optimized to detect precisely these types of changes through segmentation cost functions. Binary Segmentation, for instance, recursively partitions the series to minimize within-segment variance while maximizing between-segment differences, directly modeling the regression detection problem.

The critical role of the tolerance window $\tau$ cannot be overstated. With $\tau=5$, Binary Segmentation achieves F1=0.31, underperforming ML (F1=0.42). However, with $\tau=10$, F1 increases to 0.54, surpassing ML by 28\%. This phenomenon reflects the reality of CI/CD systems: alerts are generated retrospectively after aggregating multiple test runs, introducing inherent delays between the actual code change and alert timestamp. A tolerance window of 10 commits (approximately 4-8 hours in active repositories) accounts for this detection latency while avoiding overly permissive matching.

The perfect recall (1.00) achieved at $\tau=10$ indicates that CPD successfully identifies all alerts ultimately linked to bugs, with the precision-recall trade-off determined by $\tau$. This suggests CPD is highly sensitive to genuine regressions but requires parameter tuning to control false positives.

\subsection{The Dominance of Context: A Socio-Technical Phenomenon}
Perhaps the most surprising finding in our ablation study is that contextual features (repository, platform, framework) achieve F1=0.49 when used alone, outperforming both magnitude-only features (F1=0.40) and even the complete 21-feature set (F1=0.42). More strikingly, \textit{removing} magnitude features improves performance to F1=0.49, suggesting magnitude may introduce noise rather than signal.

This result challenges the conventional assumption that regression severity (magnitude) is the primary determinant of actionability. Instead, bug-worthiness appears to be determined more by \textit{where} an alert occurs than \textit{how large} the change is. This reflects a fundamental socio-technical reality: performance regression triage is not a purely technical decision but a human process influenced by organizational factors.

Several mechanisms explain this phenomenon:
\begin{enumerate}
    \item \textbf{Team Culture}: Different repositories have different sheriffing cultures. Firefox-Android (mobile) may have stricter performance budgets than experimental features, leading to higher bug-filing rates for the same magnitude change.
    \item \textbf{Platform Criticality}: Android and Windows platforms may receive more scrutiny due to larger user bases, while Linux regressions might be deprioritized.
    \item \textbf{Historical Noise Levels}: Some test suites exhibit higher baseline variance. Sheriffs learn to trust alerts from stable suites while dismissing noisy ones, regardless of magnitude.
    \item \textbf{Resource Constraints}: When overwhelmed with alerts, sheriffs may triage by repository importance rather than methodically analyzing each magnitude.
\end{enumerate}

This finding has profound implications: to predict which alerts will receive developer attention, one must model \textit{sheriff behavior}, not just technical severity. The optimal triage system should therefore prioritize alerts from critical repositories and stable platforms, using magnitude only as a secondary signal within each context.

\subsection{Implications for Practice}
Our findings yield actionable recommendations for performance monitoring systems:

\textbf{Hybrid Triage Architecture.} Rather than replacing supervised ML with CPD entirely, we recommend a two-stage hybrid system: (1) CPD performs real-time change detection on incoming time series, flagging structural breaks as candidate alerts; (2) A lightweight contextual classifier (using only repository, platform, framework) prioritizes these alerts for sheriff attention. This architecture combines CPD's temporal sensitivity with context-aware prioritization, avoiding the overhead of complex feature engineering while maintaining high recall.

\textbf{Context-Aware Alert Routing.} Alerts should be routed and prioritized based on their contextual features. High-priority repositories (e.g., release branches) should trigger immediate notifications, while experimental repositories can be batched. Platform-specific dashboards can help sheriffs focus on their areas of expertise, reducing cognitive load.

\textbf{Adaptive Tolerance Windows.} The optimal $\tau$ varies by repository activity. High-velocity repositories (multiple commits per hour) require smaller $\tau$ (5-7) to avoid matching spurious changes, while slower repositories can use larger $\tau$ (10-15). Systems should dynamically adjust $\tau$ based on commit frequency.

\textbf{Transfer Learning Challenges.} Our cross-repository results (Table~\ref{tab:cross_repo}) demonstrate that models trained on one repository generalize poorly to others (F1 drops from 0.42 to 0.29-0.52). Organizations adopting ML-based triage should train repository-specific models or use unsupervised CPD, which requires no training data and generalizes naturally across contexts.

\subsection{Threats to Validity}
\textbf{Internal}: We address data leakage through direction-agnostic features and temporal splits. Change-point detection algorithms were validated on both synthetic (controlled) and real-world (has\_bug prediction) data to ensure methodological rigor.

\textbf{External}: Results are specific to Mozilla's Firefox testing infrastructure and may not generalize to other projects. Cross-repository evaluation (Table~\ref{tab:cross_repo}) demonstrates limited generalization even within Mozilla, suggesting organization-specific patterns.

\textbf{Construct}: The \texttt{has\_bug} target represents human triage decisions (Performance Sheriff behavior) rather than objective regression severity. Our models predict what a Sheriff will deem actionable, which may reflect organizational culture, resource constraints, and team priorities in addition to technical severity. This is appropriate for practical deployment but differs from ground-truth performance impact measurement. Some actionable regressions may not result in filed bugs due to workload prioritization.

\section{Related Work}
Chen et al.~\cite{chen2020improving} applied supervised learning to performance alert classification with F1=0.52-0.87. Our direction-agnostic methodology prevents data leakage that may inflate reported metrics. Dang et al.~\cite{dang2019aidtriage} used deep learning for bug triage; we find simpler methods competitive. Killick et al.~\cite{killick2012optimal} introduced PELT for optimal segmentation; we extend this to CI/CD contexts.

\section{Conclusion}
We presented a comparative study of anomaly detection paradigms for performance regression triage. Our key finding is that Binary Segmentation change-point detection (F1=0.54 with $\tau$=10) outperforms supervised machine learning (F1=0.42) by 29\% for predicting bug-linked alerts. Furthermore, contextual features dominate magnitude features, suggesting organizational factors are primary determinants of alert actionability. These results advocate for unsupervised temporal methods in CI/CD performance monitoring.

\balance
\bibliographystyle{ACM-Reference-Format}
\begin{thebibliography}{9}

\bibitem{besbes2025mozilla}
Amel Besbes, Maxime Lamothe, and Foutse Khomh. 2025. The Mozilla Performance Regression Dataset. In \textit{Proc. IEEE SANER 2025}. IEEE.

\bibitem{killick2012optimal}
Rebecca Killick, Paul Fearnhead, and Idris A. Eckley. 2012. Optimal detection of changepoints with a linear computational cost. \textit{Journal of the American Statistical Association} 107, 500 (2012), 1590--1598.

\bibitem{adams2007bayesian}
Ryan Prescott Adams and David J.C. MacKay. 2007. Bayesian online changepoint detection. \textit{arXiv preprint arXiv:0710.3742} (2007).

\bibitem{chen2020improving}
Tse-Hsun Chen, Weiyi Shang, Zhen Ming Jiang, Ahmed E. Hassan, Mohamed Nasser, and Parminder Flora. 2020. Improving the automation of performance regression testing. \textit{IEEE Trans. Softw. Eng.} 46, 3 (2020), 256--283.

\bibitem{dang2019aidtriage}
Yingnong Dang, Dongmei Zhang, Song Ge, Chengyun Chu, Yingjun Qiu, and Tao Xie. 2012. ReBucket: A method for clustering duplicate crash reports based on call stack similarity. In \textit{Proc. ICSE 2012}. IEEE, 1084--1093.

\bibitem{shepperd2014data}
Martin Shepperd, David Bowes, and Tracy Hall. 2014. Researcher bias: The use of machine learning in software defect prediction. \textit{IEEE Trans. Softw. Eng.} 40, 6 (2014), 603--616.

\end{thebibliography}

\end{document}

